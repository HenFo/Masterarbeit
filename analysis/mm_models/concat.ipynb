{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import my_utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils.collator import SequenceGenerationCollator\n",
    "from utils.model import MmLlamaConcat, MmLlamaConfig\n",
    "from utils.processor import MmLlamaProcessor\n",
    "\n",
    "BASE_PATH = os.path.join(\"results\", \"early_concat\")\n",
    "INSTRUCTERC_BASE_PATH = os.path.join(\"results\", \"instructerc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden die erzielten Ergebnisse und das Early-Concatenation Modell weitergehend untersucht, um folgenden Forschungsfragen zu beantworten:\n",
    "- Wie ist die performance auf IEMOCAP und MELD\n",
    "- Gibt es Änderungen in der Klassifizierung im Vergleich zum normalen InstructERC\n",
    "  - Wenn ja, welche?\n",
    "- Was wird durch die Akustik erkannt?\n",
    "  - Was wird nur durch Akustik erkannt?\n",
    "  - Gibt es Verbesserungen in bestimmten Emotionen\n",
    "- Kann das Modell das volle Potenzial aus beiden Modalitäten ausnutzen?\n",
    "- Nutzt das Modell die neuen Feature?\n",
    "- Hat das Vortraining einen Einfluss auf die Effektivität des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IEMOCAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET = \"IEMOCAP\"\n",
    "\n",
    "\n",
    "def get_result_dataframe(dataset: str) -> pd.DataFrame:\n",
    "    normal_path = os.path.join(BASE_PATH, dataset, \"preds_test_normal.json\")\n",
    "    audio_path = os.path.join(BASE_PATH, dataset, \"preds_test_audio_only.json\")\n",
    "\n",
    "    normal_df = utils.build_result_dataframe(normal_path)\n",
    "    audio_df = utils.build_result_dataframe(audio_path)\n",
    "    ierc_df = utils.get_instructerc_results(INSTRUCTERC_BASE_PATH, dataset)\n",
    "\n",
    "    results = utils.merge_result_dataframes([normal_df, audio_df, ierc_df], [\"normal\", \"audio\", \"ierc\"])\n",
    "    results = utils.extract_dialogue_information(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "df_iemocap = get_result_dataframe(DATASET)\n",
    "\n",
    "len(df_iemocap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input_x    Now you are expert of sentiment and emotional ...\n",
       "input_y    Now you are expert of sentiment and emotional ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iemocap[[\"input_x\", \"input_y\"]].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "Dafür wird der gewichtete F1 Score berechnet:\n",
    "\\begin{equation}\n",
    "    F1 = 2\\cdot\\frac{precision\\cdot recall}{precision + recall}\n",
    "\\end{equation}\n",
    "Dieser wird für jedes Label berechnet und durch die Anzahl der label normalisiert.\n",
    "Dadurch bekommt jedes Label, unabhängig von der Anzah, die selbe Gewichtung im Gesamtergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6993065614123333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "labels = df_iemocap[\"target\"].value_counts().index.to_list()\n",
    "f1_score(df_iemocap[\"target\"], df_iemocap[\"output\"], average=\"weighted\", labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfluss der Akustik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Modell hat 160 Beispiele nicht richtig erkannt, obwohl die Akustik das richtige Ergebnis geliefert hätte\n"
     ]
    }
   ],
   "source": [
    "# TODO Merge mit instructerc, wenn der Index stimmt\n",
    "\n",
    "audio_but_not_both_correct = df_iemocap[(df_iemocap[\"target\"] == df_iemocap[\"output_audio\"]) & (df_iemocap[\"target\"] != df_iemocap[\"output\"])]\n",
    "print(f\"Das Modell hat {len(audio_but_not_both_correct)} Beispiele nicht richtig erkannt, obwohl die Akustik das richtige Ergebnis geliefert hätte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu untersuchen, ob das training effektiv und das Modell die zusätzlichen feature tatsächlich verwendet, untersuchen wir die Attention-Weight Matrix.\n",
    "Diese gibt aufschluss darüber, worauf sich das Modell konzentriert.\n",
    "Zur erinnerung, die Attention-Matrix ist eine $T_x \\times T_x$ Matrix, wobei $T_x$ für die Länge der Eingabesequenz steht.\n",
    "Diese ist besitzt nur im unteren linken Dreieck $\\{A_{i,j} \\mid i \\ge j\\}$ Werte, während alle anderen 0 betragen.\n",
    "Eine Begründung dafür steht in Abschnitt (ref...).\n",
    "Jede Zeile enthält das Attenntion-Query Ergebnis eines Query-Vektors, während die Spalten die Ergebnisse der Keys representiert.\n",
    "Zu interpretieren ist also eine Zeile, dass sich aus der neue Kontext-Vektor für den Token zu prozentualen Anteilen zusammensetzt, wie diese in der Zeile stehen.\n",
    "Währenddessen zeigt jede Zeile, wie stark ein gegebener Token zum nächsten Zustand beigetragen hat.\n",
    "Befinden sich in einer Spalte nur niedrige Werte, hat dieser Token wenig zum Ergebnisausgang beigetragen.\n",
    "\n",
    "Wenn man also überprüfen möchte, ob das Netz die akustischen Feature tatsächlich verwendet, sollte es keine besonderen Auffälligkeiten oder erhöhte Werte in den Spalten der Audio-Token geben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_MODEL = os.path.abspath(\"../../models/language/LLaMA2-base\")\n",
    "LORA_ADAPTER = os.path.abspath(\"../../models/language/adapter/iemocap/LLaMA2-base\")\n",
    "ACOUSTIC_MODEL = os.path.abspath(\n",
    "    \"../../models/acoustic/wav2vec2/wav2vec2-large-robust-12-ft-emotion-msp-dim\"\n",
    ")\n",
    "DS_TRAIN_PATH = os.path.abspath(\"../../datasets/iemocap/iemocap.csv\")\n",
    "DS_DEV_PATH = os.path.abspath(\"../../datasets/iemocap/iemocap.csv\")\n",
    "DS_TEST_PATH = os.path.abspath(\"../../datasets/iemocap/iemocap.csv\")\n",
    "STAGE1_PATH = os.path.abspath(\n",
    "    \"../../experiments/multimodal/concat/iemocap/LLaMA2-base/mlp/audio_instruction/stage_1\"\n",
    ")\n",
    "STAGE2_PATH = os.path.abspath(\n",
    "    \"../../experiments/multimodal/concat/iemocap/LLaMA2-base/mlp/audio_instruction/stage_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(\n",
    "    model: MmLlamaConcat,\n",
    "    dataset_path: str,\n",
    "    processor: MmLlamaProcessor,\n",
    "    layer_idx=0,\n",
    "    sample_index=0,\n",
    "):\n",
    "    sample = utils.get_sample(\n",
    "        dataset_path,\n",
    "        processor,\n",
    "        [sample_index],\n",
    "        collator_type=SequenceGenerationCollator,\n",
    "        dataset_kwargs={\"task\": \"normal\", \"audio_placement\": \"target\"},\n",
    "    )\n",
    "\n",
    "    llama = model.llama\n",
    "    att1 = llama.get_submodule(f\"model.layers.{layer_idx}.self_attn\")\n",
    "\n",
    "    attention_weights = None\n",
    "\n",
    "    def attention_hook(module, input, output):\n",
    "        nonlocal attention_weights\n",
    "        attention_weights = output[1]\n",
    "\n",
    "    att_handle = att1.register_forward_hook(attention_hook)\n",
    "\n",
    "    def prepate_nested_batch(batch: dict[dict[torch.Tensor]]):\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "\n",
    "        text = {k: v.to(device) for k, v in batch[\"text\"].items()}\n",
    "        acoustic = {k: v.half().to(device) for k, v in batch[\"acoustic\"].items()}\n",
    "\n",
    "        return {**text, **acoustic}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(**prepate_nested_batch(sample))\n",
    "\n",
    "    att_handle.remove()\n",
    "    return attention_weights[0].cpu().numpy()\n",
    "\n",
    "\n",
    "def print_attention_weight_matrix(\n",
    "    attention_weights: np.ndarray,\n",
    "    sample,\n",
    "    config: MmLlamaConfig,\n",
    "    tokenizer: AutoTokenizer,\n",
    "):\n",
    "    head_norm = attention_weights.mean(axis=0)\n",
    "    head_norm = np.apply_along_axis(lambda x: x / np.max(x), 1, head_norm)\n",
    "\n",
    "    audio_token_id = config.audio_token_id\n",
    "\n",
    "    token_ids = sample[\"text\"][\"input_ids\"][0].cpu().numpy()\n",
    "    audio_loc = np.where(token_ids == audio_token_id)[0][0]\n",
    "    token_ids = np.concatenate(\n",
    "        (token_ids[:audio_loc], [audio_token_id] * 10, token_ids[audio_loc:])\n",
    "    )\n",
    "    token_strings = np.array(tokenizer.convert_ids_to_tokens(token_ids))\n",
    "    # token_strings[token_ids == 0] = \"\"\n",
    "\n",
    "    mean_attention = np.mean(head_norm, axis=0, where=head_norm != 0)\n",
    "    # print(mean_attention)\n",
    "    top_token_ids_idx = np.argsort(mean_attention)[::-1][:25]\n",
    "    # print(top_token_ids_idx)\n",
    "    # last_token_ids_idx = np.argsort(mean_attention)[:10]\n",
    "    audio_loc_idx = np.where(token_ids == audio_token_id)[0]\n",
    "    selected_tokens = np.unique(np.concatenate([top_token_ids_idx, audio_loc_idx]))\n",
    "    token_strings[~np.isin(np.arange(len(token_strings)), selected_tokens)] = \"\"\n",
    "    # print(token_strings)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(head_norm, cmap=\"viridis\")\n",
    "    plt.xticks(range(len(token_strings)), token_strings, rotation=90, fontsize=6)\n",
    "    plt.yticks(range(len(token_strings)), token_strings, fontsize=6)\n",
    "    plt.xlabel(\"Query Tokens\")\n",
    "    plt.ylabel(\"Key Tokens\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_attention_weights(dataset_path: str, layer_idx=0, sample_index=0):\n",
    "    model, config, processor = utils.get_model(\n",
    "        LANGUAGE_MODEL, LORA_ADAPTER, ACOUSTIC_MODEL, STAGE1_PATH, MmLlamaConcat, {\"output_attention_weights\":True}\n",
    "    )\n",
    "    attention_weights = get_attention_weights(\n",
    "        model, dataset_path, layer_idx, sample_index\n",
    "    )\n",
    "    print_attention_weight_matrix(\n",
    "        attention_weights, sample_index, config, processor.tokenizer\n",
    "    )\n",
    "\n",
    "\n",
    "print_model_attention_weights(layer_idx=0, sample_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um herauszufinden, welchen Einfluss das Vortraining auf die Verteilung der GEwichte hat, ist hier die Matrix eines Modells, welches direkt auf Stage 3 trainiert wurde."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
