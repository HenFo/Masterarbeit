{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoProcessor, AutoTokenizer\n",
    "\n",
    "from utils.collator import SequenceGenerationCollator\n",
    "from utils.dataset import ERCDataset, IemocapDataset, MeldDataset\n",
    "from utils.model import MmLlamaConcat, MmLlamaConfig\n",
    "from utils.processor import MmLlamaProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden die erzielten Ergebnisse und das Early-Concatenation Modell weitergehend untersucht, um folgenden Forschungsfragen zu beantworten:\n",
    "- Wie ist die performance auf IEMOCAP und MELD\n",
    "- Gibt es Änderungen in der Klassifizierung im Vergleich zum normalen InstructERC\n",
    "  - Wenn ja, welche?\n",
    "- Was wird durch die Akustik erkannt?\n",
    "  - Was wird nur durch Akustik erkannt?\n",
    "  - Gibt es Verbesserungen in bestimmten Emotionen\n",
    "- Kann das Modell das volle Potenzial aus beiden Modalitäten ausnutzen?\n",
    "- Nutzt das Modell die neuen Feature?\n",
    "- Hat das Vortraining einen Einfluss auf die Effektivität des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu untersuchen, ob das training effektiv und das Modell die zusätzlichen feature tatsächlich verwendet, untersuchen wir die Attention-Weight Matrix.\n",
    "Diese gibt aufschluss darüber, worauf sich das Modell konzentriert.\n",
    "Zur erinnerung, die Attention-Matrix ist eine $T_x \\times T_x$ Matrix, wobei $T_x$ für die Länge der Eingabesequenz steht.\n",
    "Diese ist besitzt nur im unteren linken Dreieck $\\{A_{i,j} \\mid i \\ge j\\}$ Werte, während alle anderen 0 betragen.\n",
    "Eine Begründung dafür steht in Abschnitt (ref...).\n",
    "Jede Zeile enthält das Attenntion-Query Ergebnis eines Query-Vektors, während die Spalten die Ergebnisse der Keys representiert.\n",
    "Zu interpretieren ist also eine Zeile, dass sich aus der neue Kontext-Vektor für den Token zu prozentualen Anteilen zusammensetzt, wie diese in der Zeile stehen.\n",
    "Währenddessen zeigt jede Zeile, wie stark ein gegebener Token zum nächsten Zustand beigetragen hat.\n",
    "Befinden sich in einer Spalte nur niedrige Werte, hat dieser Token wenig zum Ergebnisausgang beigetragen.\n",
    "\n",
    "Wenn man also überprüfen möchte, ob das Netz die akustischen Feature tatsächlich verwendet, sollte es keine besonderen Auffälligkeiten oder erhöhte Werte in den Spalten der Audio-Token geben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_MODEL = os.path.abspath(\"../../models/language/LLaMA2-base\")\n",
    "LORA_ADAPTER = os.path.abspath(\"../../models/language/adapter/iemocap/LLaMA2-base\")\n",
    "ACOUSTIC_MODEL = os.path.abspath(\n",
    "    \"../../models/acoustic/wav2vec2/wav2vec2-large-robust-12-ft-emotion-msp-dim\"\n",
    ")\n",
    "DS_TRAIN_PATH = os.path.abspath(\"../../datasets/iemocap/iemocap.csv\")\n",
    "DS_DEV_PATH = os.path.abspath(\"../../datasets/iemocap/iemocap.csv\")\n",
    "DS_TEST_PATH = os.path.abspath(\"../../datasets/iemocap/iemocap.csv\")\n",
    "STAGE1_PATH = os.path.abspath(\n",
    "    \"../../experiments/multimodal/concat/iemocap/LLaMA2-base/mlp/audio_instruction/stage_1\"\n",
    ")\n",
    "STAGE2_PATH = os.path.abspath(\n",
    "    \"../../experiments/multimodal/concat/iemocap/LLaMA2-base/mlp/audio_instruction/stage_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "config = None\n",
    "processor = None\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    llm_path: str, adapter_path: str, acoustic_path: str, checkpoint_path: str\n",
    "):\n",
    "    global model, config, processor\n",
    "    if None in (model, config, processor):\n",
    "        return model, config, processor\n",
    "\n",
    "    llm_config = AutoConfig.from_pretrained(llm_path)\n",
    "    ac_config = AutoConfig.from_pretrained(acoustic_path)\n",
    "    ac_processor = AutoProcessor.from_pretrained(acoustic_path)\n",
    "\n",
    "    # setup of tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<audio>\"]})\n",
    "    tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # setup of processor\n",
    "    processor = MmLlamaProcessor(ac_processor, tokenizer)\n",
    "\n",
    "    ## setup of config\n",
    "    audio_token_id = tokenizer.additional_special_tokens_ids[0]\n",
    "    config = MmLlamaConfig(\n",
    "        llm_config=llm_config,\n",
    "        audio_config=ac_config,\n",
    "        audio_token_id=audio_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        llm_pretrained_adapter=adapter_path,\n",
    "        num_labels=0,\n",
    "    )\n",
    "\n",
    "    model = MmLlamaConcat(config, output_attention_weights=True)\n",
    "    model.load_state_dict(\n",
    "        torch.load(os.path.join(checkpoint_path, \"best_model.pth\")), strict=False\n",
    "    )\n",
    "    model = model.apply_inference_lora(checkpoint_path)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    return model, config, processor\n",
    "\n",
    "\n",
    "def get_sample(dataset_path: str, sample_index=0):\n",
    "    def dataset_class(dataset_path: str) -> Type[ERCDataset]:\n",
    "        if \"meld\" in dataset_path:\n",
    "            return MeldDataset\n",
    "        if \"iemocap\" in dataset_path:\n",
    "            return IemocapDataset\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dataset path\")\n",
    "\n",
    "    test_dataset = dataset_class(dataset_path)(\n",
    "        dataset_path, mode=\"test\", task=\"normal\", audio_placement=\"target\"\n",
    "    )\n",
    "    raw_sample = test_dataset[sample_index]\n",
    "    sample = SequenceGenerationCollator(processor, mode=\"dev\")([raw_sample])\n",
    "    sample = sample[0]\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def get_attention_weights(\n",
    "    model: MmLlamaConcat, dataset_path: str, layer_idx=0, sample_index=0\n",
    "):\n",
    "    sample = get_sample(dataset_path, sample_index)\n",
    "\n",
    "    llama = model.llama\n",
    "    att1 = llama.get_submodule(f\"model.layers.{layer_idx}.self_attn\")\n",
    "\n",
    "    attention_weights = None\n",
    "\n",
    "    def attention_hook(module, input, output):\n",
    "        nonlocal attention_weights\n",
    "        attention_weights = output[1]\n",
    "\n",
    "    att_handle = att1.register_forward_hook(attention_hook)\n",
    "\n",
    "    def prepate_nested_batch(batch: dict[dict[torch.Tensor]]):\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "\n",
    "        text = {k: v.to(device) for k, v in batch[\"text\"].items()}\n",
    "        acoustic = {k: v.half().to(device) for k, v in batch[\"acoustic\"].items()}\n",
    "\n",
    "        return {**text, **acoustic}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(**prepate_nested_batch(sample))\n",
    "\n",
    "    att_handle.remove()\n",
    "    return attention_weights[0].cpu().numpy()\n",
    "\n",
    "\n",
    "def print_attention_weight_matrix(\n",
    "    attention_weights: np.ndarray,\n",
    "    sample,\n",
    "    config: MmLlamaConfig,\n",
    "    tokenizer: AutoTokenizer,\n",
    "):\n",
    "    head_norm = attention_weights.mean(axis=0)\n",
    "    head_norm = np.apply_along_axis(lambda x: x / np.max(x), 1, head_norm)\n",
    "\n",
    "    audio_token_id = config.audio_token_id\n",
    "\n",
    "    token_ids = sample[\"text\"][\"input_ids\"][0].cpu().numpy()\n",
    "    audio_loc = np.where(token_ids == audio_token_id)[0][0]\n",
    "    token_ids = np.concatenate(\n",
    "        (token_ids[:audio_loc], [audio_token_id] * 10, token_ids[audio_loc:])\n",
    "    )\n",
    "    token_strings = np.array(tokenizer.convert_ids_to_tokens(token_ids))\n",
    "    # token_strings[token_ids == 0] = \"\"\n",
    "\n",
    "    mean_attention = np.mean(head_norm, axis=0, where=head_norm != 0)\n",
    "    # print(mean_attention)\n",
    "    top_token_ids_idx = np.argsort(mean_attention)[::-1][:25]\n",
    "    # print(top_token_ids_idx)\n",
    "    # last_token_ids_idx = np.argsort(mean_attention)[:10]\n",
    "    audio_loc_idx = np.where(token_ids == audio_token_id)[0]\n",
    "    selected_tokens = np.unique(np.concatenate([top_token_ids_idx, audio_loc_idx]))\n",
    "    token_strings[~np.isin(np.arange(len(token_strings)), selected_tokens)] = \"\"\n",
    "    # print(token_strings)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(head_norm, cmap=\"viridis\")\n",
    "    plt.xticks(range(len(token_strings)), token_strings, rotation=90, fontsize=6)\n",
    "    plt.yticks(range(len(token_strings)), token_strings, fontsize=6)\n",
    "    plt.xlabel(\"Query Tokens\")\n",
    "    plt.ylabel(\"Key Tokens\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_attention_weights(dataset_path: str, layer_idx=0, sample_index=0):\n",
    "    model, config, processor = get_model(\n",
    "        LANGUAGE_MODEL, LORA_ADAPTER, ACOUSTIC_MODEL, STAGE1_PATH\n",
    "    )\n",
    "    attention_weights = get_attention_weights(\n",
    "        model, dataset_path, layer_idx, sample_index\n",
    "    )\n",
    "    print_attention_weight_matrix(\n",
    "        attention_weights, sample_index, config, processor.tokenizer\n",
    "    )\n",
    "\n",
    "\n",
    "print_model_attention_weights(layer_idx=0, sample_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um herauszufinden, welchen Einfluss das Vortraining auf die Verteilung der GEwichte hat, ist hier die Matrix eines Modells, welches direkt auf Stage 3 trainiert wurde."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
