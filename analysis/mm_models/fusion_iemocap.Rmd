---
title: An R Markdown document converted from "fusion_iemocap.ipynb"
output: html_document
---

```{python}
import os
import sys

sys.path.append(os.path.abspath("../../"))

import matplotlib.pyplot as plt
import my_utils as utils
import numpy as np
import pandas as pd

plt.rcParams["figure.figsize"] = (10, 12)
plt.rcParams["figure.dpi"] = 96

BASE_PATH = os.path.join("results", "late_fusion")
INSTRUCTERC_BASE_PATH = os.path.join("results", "InstructERC")
```

# Late Fusion

In diesem Abschnitt werden die erzielten Ergebnisse und das Late-Fusion Modells weitergehend untersucht, um folgenden Forschungsfragen zu beantworten:
- Wie ist die performance auf IEMOCAP und MELD
- Gibt es Änderungen in der Klassifizierung im Vergleich zum normalen InstructERC
  - Wenn ja, welche?
- Was wird durch die Akustik erkannt?
  - Was wird nur durch Akustik erkannt?
  - Gibt es Verbesserungen in bestimmten Emotionen
- Kann das Modell das volle Potenzial aus beiden Modalitäten ausnutzen?
- Wie verhält sich das Gating-modul?
  - was passiert bei verrauschtem Audio/Text Input?
- passt das alignment der beiden Feature vectoren bei gleicher Emotion?
  - muss die Klassifizierungsschicht zwei Latenzräume lernen?

## IEMOCAP


```{python}
DATASET = "IEMOCAP"


def get_result_dataframe(dataset: str) -> pd.DataFrame:
    normal_path = os.path.join(BASE_PATH, dataset, "preds_test.json")
    audio_path = os.path.join(BASE_PATH, dataset, "preds_test_no_text.json")
    text_path = os.path.join(BASE_PATH, dataset, "preds_test_no_audio.json")

    normal_df = utils.build_result_dataframe(normal_path)
    audio_df = utils.build_result_dataframe(audio_path)
    text_df = utils.build_result_dataframe(text_path)
    ierc_df = utils.get_instructerc_results(INSTRUCTERC_BASE_PATH, dataset)

    assert len(normal_df) == len(audio_df) == len(ierc_df) == len(text_df)

    results = utils.merge_result_dataframes([normal_df, audio_df, ierc_df, text_df], ["normal", "audio", "ierc", "text"])
    results = utils.extract_dialogue_information(results)

    return results


df = get_result_dataframe(DATASET)
df.head(3)
```

```{python}
df["output_ierc"].value_counts()
df[df["output_ierc"].isin(["grateful", "embarrassed"])]
```

### Performance

Dafür wird der gewichtete F1 Score berechnet:
\begin{equation}
    F1 = 2\cdot\frac{precision\cdot recall}{precision + recall}
\end{equation}
Dieser wird für jedes Label berechnet und durch die Anzahl der label normalisiert.
Dadurch bekommt jedes Label, unabhängig von der Anzah, die selbe Gewichtung im Gesamtergebnis.

```{python}
from sklearn.metrics import f1_score

labels = df["target"].value_counts().index.to_list()

df["any_correct"] = df[
    (df["output_ierc"] == df["target"]) | (df["output_audio"] == df["target"]) | (df["output"] == df["target"])
]["target"]
df.loc[df["any_correct"].isna(), "any_correct"] = df.loc[
    df["any_correct"].isna(), "output_ierc"
]

fusion_model_f1 = f1_score(
    df["target"], df["output"], average="weighted", labels=labels
)
audio_only_f1 = f1_score(
    df["target"], df["output_audio"], average="weighted", labels=labels
)
text_only_f1 = f1_score(
    df["target"], df["output_ierc"], average="weighted", labels=labels
)
ideal_f1 = f1_score(df["target"], df["any_correct"], average="weighted", labels=labels)

print(f"F1-Score des Fusion Modells: {100*fusion_model_f1:.2f}")
print(f"F1-Score mit nur audio input: {100*audio_only_f1:.2f}")
print(f"F1-Score des Basismodells: {100*text_only_f1:.2f}")
print(f"Änderung: {(100*fusion_model_f1 - 100*text_only_f1):.2f}")
print(f"Idealer F1-Score: {100*ideal_f1:.2f}")
```

Betrachten wir uns nun die Confusion-Matrix des fusionination Modells.
In Klammern ist der prozentuale Anteil der Zeile angegeben. Daran orientiert sich ebenfalls auch die Farbgebung.

```{python}
utils.print_confusion_matrix(df, target_labels=labels[::-1], output_column="output", name = "images/fusion_iemocap_combined_cm.png", title="Confusion Matrix on IEMOCAP")
```

```{python}
# confusion_matrix(df["target"], df["output"], labels=labels)
labels
```

Auffälligkeiten:
Es können entgegengesetzte Emotionen definiert werden. Die eine Klasse bilden die positiven Emotionen "happy" und "excited, die andere negative Emotionen "sad", "angry" und "frustrated".
Zwischen diesen beiden Gruppen gibt es so gut wie keine Verwechselungen, wie die Confusion-Matrix, mit einteilung in positiv und negativ, zeigt.

```{python}
from functools import partial

iemocap_positive_emotions = ["happy", "excited"]
iemocap_negative_emotions = ["angry", "sad", "frustrated"]


iemocap_classify_sentiment = partial(utils.classify_sentiment, positive=iemocap_positive_emotions, negative=iemocap_negative_emotions)

df["emotion_sentiment"] = df["target"].apply(iemocap_classify_sentiment)
df["emotion_sentiment_prediction"] = df["output"].apply(iemocap_classify_sentiment)


utils.print_confusion_matrix(df, target_labels=["positive", "negative"], output_column="emotion_sentiment_prediction", target_column="emotion_sentiment")
```

Insgesammt werden nur 20 von 1110 Beispiele, also $\sim$ 1,18% zwischen den Gruppen falsch zugeordnet.

Anders sieht es bei der Intra-Klassenverwechselung aus:

```{python}
utils.print_confusion_matrix(
    df,
    target_labels=iemocap_negative_emotions,
    output_column="output",
    target_column="target",
)
```

In den negativen Emotionen kommt es insbesondere bei Wut und Frustration zu Verwechselungen.
Trauer kann sehr gut von Wut auseinandergehalten werden, hier kommt es kaum zu Verwechselungen.

```{python}
utils.print_confusion_matrix(
    df,
    target_labels=iemocap_positive_emotions,
    output_column="output",
    target_column="target",
)
```

In der positiven Gruppe kommt es zu ähnlich häufigen Verwechselungen zwischen Aufgeregt udn Glücklich.
Zu beachten ist aber, dass "Glücklich" mit 143 Beispielen die am wenigsten vertretene Emotion im Datensatz.

TODO: Gucken, wie oft happy für excited von mindestens einem annotator vorgeschlagen wurde.

Weiter wird untersucht, worin die Veränderungen bestehen.
Geguckt wird, wieviele Beispiele jetzt nicht mehr erkannt werden und wie viele jetzt erkannt werden.

```{python}
combined_model = df["target"] == df["output"]  # C
text_only_model = df["target"] == df["output_ierc"]  # T
audio_only_model = df["target"] == df["output_audio"]  # A

# Define negations
not_combined_model = ~combined_model
not_text_only_model = ~text_only_model
not_audio_only_model = ~audio_only_model

both_correct_but_not_text = df[(combined_model) & (not_text_only_model)]
text_correct_but_not_both = df[(text_only_model) & (not_combined_model)]


print(f"Nach dem Hinzufügen der Akustik können {len(both_correct_but_not_text)} Beispiele erkannt werden, die vorher nicht erkannt wurden")
print(f"Nach dem Hinzufügen der Akustik können {len(text_correct_but_not_both)} Beispiele nicht mehr richtig erkannt werden")
```

Es zeigt sich, dass das Modell fast so viele Beispiele nicht mehr richtig erkennt, wie es jetzt neu erkennt.
Dies erklärt den nur leicht gestiegenen F1-Score.

#### Veränderung der Emotionen

Wie sieht die Veränderung zwischen den Emotionen aus?
Dafür werden zuerst die F1-Scores im Vergleich vorher - nachher gezeigt, um zu sehen, ob es Veränderungen in den einzelnen Emotionen gab.

```{python}
for label in labels:
    f1_label = f1_score(df["target"], df["output"], labels=[label], average="weighted") * 100
    f1_label_ierc = f1_score(df["target"], df["output_ierc"], labels=[label], average="weighted") * 100
    
    print(f"F1-Score für Label {label}: {f1_label:.2f} - Vorher: {f1_label_ierc:.2f} - Differenz: {(f1_label - f1_label_ierc):.2f}")
```

Es gibt leichte Verbesserungen in den Klassen "neutral", "excited", "sad", während leichte Verschlechterungen in den Klassen "frustrated" und "happy" vorkommen.
"angry" hat einen nahezu gleichen Wert.

Als nächstes werden die Änderungen in der Confusion-Matrix analysiert.

```{python}
utils.print_confusion_matrix_difference(df, target_labels=labels[::-1], output_column1="output", output_column2="output_ierc", name = "images/fusion_iemocap_diff_cm.png", title="Confusion Matrix difference on IEMOCAP")
```

```{python}
from sklearn.metrics import confusion_matrix

print(labels)
confusion_matrix(df["target"], df["output"], labels=labels) - confusion_matrix(df["target"], df["output_ierc"], labels=labels)
```

```{python}
from sklearn.metrics import precision_score, recall_score

for label in labels:
    y_true = df["target"].apply(lambda x: 1 if x == label else 0)
    y_pred = df["output"].apply(lambda x: 1 if x == label else 0)
    y_pred_ierc = df["output_ierc"].apply(lambda x: 1 if x == label else 0)

    pre_label = precision_score(y_true, y_pred) * 100
    rec_label = recall_score(y_true, y_pred) * 100
    pre_label_ierc = precision_score(y_true, y_pred_ierc) * 100
    rec_label_ierc = recall_score(y_true, y_pred_ierc) * 100
    
    print(f"Precision\t für Label {label}: {pre_label:.2f} - Vorher: {pre_label_ierc:.2f} - Differenz: {(pre_label - pre_label_ierc):.2f}")
    print(f"Recall\t\t für Label {label}: {rec_label:.2f} - Vorher: {rec_label_ierc:.2f} - Differenz: {(rec_label - rec_label_ierc):.2f}")
    print()
```

Hier lässt sich erkennen, wie die Feränderungen in den F1-Scores zustandegekommen sind.
"Frustration" verliert am meisten Punkte, da es häufiger als "Wut" oder "Trauer" klassifiziert wird.
Dagegen werden "Trauer" und "Wut" jetzt weniger als "Frustration" klassifiziert, dafür häufiger korrekt.
Auch "Aufgeregt" kann nun besser zu einer neutralen Emotion differenziert werden.
Insgesamt kann besser zwischen "Neutral" / keine Emotion, und einer Emotionsklasse differenziert werden.

Wir gucken nun, ob das an der Akustik liegen kann.
- Dafür gucken wir uns die Verteilung von richtig klassifizierten Emotionen einmal aus dem "nur-Text" und einmal dem "nur-Audio" modell an.
- BEtrachten der Beispiele, die vorher falsch und jetzt richtig klassifiziert wurden. Gibt es bei der Akustik hier auch eine erhöhte Trefferrate?

```{python}

text_only_false_but_now_correct = df[(not_text_only_model) & (combined_model)]
text_only_false_but_audio_correct = df[
    (not_text_only_model) & (audio_only_model)
]

correct_by_model_and_audio_but_not_text = df[
    combined_model & audio_only_model & not_text_only_model
]

text_correct_but_not_audio_or_both = df[
    text_only_model & not_audio_only_model & not_combined_model
]
text_correct_bot_not_audio = df[text_only_model & not_audio_only_model]

print(
    f"Nach dem Hinzufügen der Akustik können {len(text_only_false_but_now_correct)} Beispiele erkannt werden, die vorher nicht erkannt wurden"
)
print(
    f"Die Akustik kann {len(text_only_false_but_audio_correct)} Beispiele erkennen, die vom Text allein nicht erkannt werden"
)
print(
    f"Insgesamt gibt es {len(correct_by_model_and_audio_but_not_text)} Beispiele, die nicht vom Text, aber vom Modell und der Akustik erkannt werden"
)
print(
    f"Insgesamt gibt es {len(text_correct_but_not_audio_or_both)} Beispiele, die nur vom Text, nicht aber vom Modell und der Akustik erkannt werden"
)
print(
    f"InstructERC kann {len(text_correct_bot_not_audio)} Beispiele richtig klassifizieren, die die Akustik nicht klassifizieren kann"
)
```

Um zu gucken, ob die Akustik Anteile an der "positiven" Veränderung in der Klassifizierung hat, wird der Anteil der Emotionen berechnet aus der Untermenge der Beispiele, die nicht vom InstructERC Modell aber von der Akustik und dem gemischten Modell richtig klassifiziert werden konnten.

```{python}
from plotnine import (
    aes,
    element_text,
    geom_col,
    geom_text,
    ggplot,
    labs,
    position_dodge,
    scale_fill_manual,
    theme,
    theme_bw,
    xlab,
    ylab,
    scale_fill_brewer
)

correct_by_model_and_audio_but_not_text = df[
    (combined_model) & (not_text_only_model) & (audio_only_model)
]

correct_by_model_and_text_but_not_audio = df[
    combined_model & not_audio_only_model & text_only_model
]

correct_by_model_only = df[
    (combined_model) & (not_audio_only_model) & (not_text_only_model)
]


proportions = correct_by_model_and_audio_but_not_text.value_counts(
    "target", normalize=True
).to_frame(name="Audio")
proportions = (
    proportions.merge(
        correct_by_model_and_text_but_not_audio.value_counts(
            "target", normalize=True
        ).to_frame(name="Text"),
        how="outer",
        on="target",
    )
    .merge(
        correct_by_model_only.value_counts("target", normalize=True).to_frame(
            name="Combined"
        ),
        how="outer",
        on="target",
    )
    .reset_index()
)
proportions = proportions.melt(
    id_vars="target",
    value_vars=["Audio", "Text", "Combined"],
    var_name="source",
    value_name="proportion",
)

proportions["proportion"] = proportions["proportion"].fillna(0.0)
proportions["target"] = pd.Categorical(
    proportions["target"],
    categories=proportions[proportions["source"] == "Audio"].sort_values(
        by="proportion", ascending=False
    )["target"],
)


p = (
    ggplot(proportions, aes(x="factor(target)", y="proportion", fill="source"))
    + geom_col(position="dodge", color="black")
    + geom_text(
        aes(label="round(proportion, 2)"),
        va="bottom",
        position=position_dodge(0.9),
        size=10,
    )
    + ylab("Proportion")
    + xlab("Emotion")
    + labs(fill="Source")
    + theme_bw()
    + theme(
        axis_text_x=element_text(size=14),
        axis_text_y=element_text(size=12),
        axis_title=element_text(size=16),
        legend_text=element_text(size=14),
        legend_title=element_text(size=16),
    )
    + scale_fill_manual(values=["#1f77b4", "#ff7f0e", "#2ca02c"])
)

p.show()
p.save("images/fusion_iemocap_proportions.png", width=10, height=5)
# proportions
```

```{python}
from scipy.stats import pearsonr

# pearsonr(proportions[proportions["source"] == "Combined"]["proportion"], proportions[proportions["source"] == "Text"]["proportion"])
pearsonr(proportions[proportions["source"] == "Combined"]["proportion"], proportions[proportions["source"] == "Audio"]["proportion"])
```

Wir sahen verbesserungen im Bereich "Excited", "Sad" und "Angry". Anhand der Verteilung exklusiv richtig klassifizierten Emotionen zeigt sich, dass die Akustik große Stärken im Bereich "Frustration" besitzt.
Aber vorallem in den Klassen "Sad" und "Angry" ist die Akustik besser als der Text, was einen Hinweis liefert, dass durch die Hinzunahme der Akustik, diese Klassen besser differenziert werden konnten.

### Idealer F1 Score

```{python}
df["any_correct"] = df[
    text_only_model | audio_only_model | combined_model
]["target"]
df.loc[df["any_correct"].isna(), "any_correct"] = df.loc[
    df["any_correct"].isna(), "output"
]

f1 = f1_score(df["target"], df["output"], average="weighted")
text_f1 = f1_score(df["target"], df["output_ierc"], average="weighted")
audio_f1 = f1_score(
    df["target"], df["output_audio"], average="weighted"
)
ideal_f1 = f1_score(df["target"], df["any_correct"], average="weighted")

print(f"Model F1 Score: {f1:.4f}")
print(f"Text F1 Score: {text_f1:.4f}")
print(f"Audio F1 Score: {audio_f1:.4f}")
print(f"Idealer F1 Score: {ideal_f1:.4f}")
```

### Weitere Statistiken
Hier werden weitere Statistiken genannt, wie das Klassifizierungsverhalten der einzelnen Teilmodelle ist.
Es wurden einmal die Ergebnisse von InstructERC, dem Late-Fusion Modell audio output, und dem kombinierten Modell berechnet

```{python}
both_correct = df[combined_model]
both_not_correct = df[not_combined_model]
text_correct = df[text_only_model]
text_not_correct = df[not_text_only_model]
text_correct_but_not_both = df[text_only_model & not_combined_model]
both_correct_but_not_text = df[combined_model & not_text_only_model]
text_and_both_correct = df[text_only_model & combined_model]
text_and_both_not_correct = df[not_text_only_model & not_combined_model]


print(f"Vom kombinierten modell konnten {len(both_correct)} Beispiele erkannt werden")
print(f"Vom kombinierten modell konnten {len(both_not_correct)} Beispiele nicht erkannt werden")
print(f"Vom Text konnten {len(text_correct)} Beispiele erkannt werden")
print(f"Vom Text konnten {len(text_not_correct)} Beispiele nicht erkannt werden")
print(f"Vom Text konnten {len(text_correct_but_not_both)} Beispiele erkannt werden, die nicht vom kombinierten erkannt wurden")
print(f"Vom kombinierten konnten {len(both_correct_but_not_text)} Beispiele erkannt werden, die nicht vom text erkannt wurden")
print(f"Vom Text und kombinierten konnten {len(text_and_both_correct)} gemiensame Beispiele erkannt werden")
```

```{python}
# Existing statistics
text_correct_bot_not_audio = df[(text_only_model) & (not_audio_only_model)]
audio_only_correct = df[(not_text_only_model) & (audio_only_model)]
both_only_correct = df[
    (combined_model) & (not_audio_only_model) & (not_text_only_model)
]
text_correct_but_not_both = df[(text_only_model) & (not_combined_model)]
audio_correct_but_not_both = df[(audio_only_model) & (not_combined_model)]
text_correct_but_not_audio_and_both = df[
    (text_only_model) & (not_audio_only_model) & (not_combined_model)
]
audio_correct_but_not_text_and_both = df[
    (audio_only_model) & (not_text_only_model) & (not_combined_model)
]

# Print existing statistics
print(
    f"Es konnten {len(text_correct_bot_not_audio)} Datenpunkte nur vom text-only Modell erkannt werden, nicht aber vom audio-only."
)
print(
    f"Es konnten {len(audio_only_correct)} Datenpunkte nur vom audio-only Modell erkannt werden, nicht aber vom text-only."
)
print(
    f"Es konnten {len(both_only_correct)} Datenpunkte nur durch gemeinsamen Input erkannt werden."
)
print(
    f"Es konnten {len(text_correct_but_not_both)} Datenpunkte nur durch text-only erkannt werden, nicht aber durch gemeinsamen Input."
)
print(
    f"Es konnten {len(audio_correct_but_not_both)} Datenpunkte nur durch audio-only erkannt werden, nicht aber durch gemeinsamen Input."
)
print(
    f"Es konnten {len(text_correct_but_not_audio_and_both)} Datenpunkte nur durch text-only erkannt werden, aber weder durch audio-only noch durch gemeinsamen Input."
)
print(
    f"Es konnten {len(audio_correct_but_not_text_and_both)} Datenpunkte nur durch audio-only erkannt werden, aber weder durch text-only noch durch gemeinsamen Input."
)

# Additional statistics

# 1. Alle Modelle haben die Zielvorhersage korrekt erkannt
all_correct = df[(text_only_model) & (audio_only_model) & (combined_model)]
print(
    f"Es konnten {len(all_correct)} Datenpunkte von allen Modellen korrekt erkannt werden. (Schnittmenge)"
)

# 2. Keines der Modelle hat die Zielvorhersage korrekt erkannt
all_incorrect = df[
    (not_text_only_model) & (not_audio_only_model) & (not_combined_model)
]
print(
    f"Es konnten {len(all_incorrect)} Datenpunkte von keinem Modell korrekt erkannt werden."
)

# 3. Text-only und Audio-only Modelle sind korrekt, aber das kombinierte Modell ist falsch
text_and_audio_correct_but_not_combined = df[
    (text_only_model) & (audio_only_model) & (not_combined_model)
]
print(
    f"Es konnten {len(text_and_audio_correct_but_not_combined)} Datenpunkte nur durch text-only und audio-only korrekt erkannt werden, aber nicht durch gemeinsamen Input."
)

# 4. Text-only und kombiniert Modelle sind korrekt, aber das Audio-only Modell ist falsch
text_and_combined_correct_but_not_audio = df[
    (text_only_model) & (combined_model) & (not_audio_only_model)
]
print(
    f"Es konnten {len(text_and_combined_correct_but_not_audio)} Datenpunkte nur durch text-only und gemeinsamen Input korrekt erkannt werden, aber nicht durch audio-only."
)

# 5. Audio-only und kombiniert Modelle sind korrekt, aber das Text-only Modell ist falsch
audio_and_combined_correct_but_not_text = df[
    (audio_only_model) & (combined_model) & (not_text_only_model)
]
print(
    f"Es konnten {len(audio_and_combined_correct_but_not_text)} Datenpunkte nur durch audio-only und gemeinsamen Input korrekt erkannt werden, aber nicht durch text-only."
)

not_combined_but_text_or_audio = df[
    (not_combined_model) & ((text_only_model) | (audio_only_model))
]

print(
    f"Es konnten {len(not_combined_but_text_or_audio)} Datenpunkte nicht durch gemeinsamen Input erkannt werden, obwohl entweder audio-only oder text-only es erkennen konnten"
)
```

#### 1. **Leistung der einzelnen Modelle:**
- **Text-only Modell:**
  - Es konnten **566 Datenpunkte** korrekt erkannt werden, die das audio-only Modell nicht richtig klassifiziert hat.
  - **101 Datenpunkte** wurden nur durch das text-only Modell korrekt erkannt, weder durch das audio-only noch durch das kombinierte Modell.
  - **465 Datenpunkte** wurden sowohl vom text-only Modell als auch vom kombinierten Modell richtig klassifiziert, jedoch nicht vom audio-only Modell.

  **Interpretation:** Das text-only Modell ist deutlich dominierend und kann viele Datenpunkte richtig klassifizieren, bei denen das audio-only Modell scheitert. 
  

- **Audio-only Modell:**
  - **172 Datenpunkte** wurden vom audio-only Modell korrekt erkannt, jedoch nicht vom text-only Modell.
  - **96 Datenpunkte** wurden nur vom audio-only Modell korrekt erkannt, aber weder vom text-only Modell noch vom kombinierten Modell.
  - **76 Datenpunkte** wurden sowohl vom audio-only als auch vom kombinierten Modell richtig erkannt, jedoch nicht vom text-only Modell.

  **Interpretation:** Das audio-only Modell kann Datenpunkte erkennen, die für das text-only Modell schwer zugänglich sind, jedoch insgesamt deutlich weniger als das text-only Modell. Die Tatsache, dass nur **96 Datenpunkte** von **172** Datenpunkten korrekt vom gemeinsamen Modell erkannt wurden, zwiegt, dass die Stärken der Akustik nicht gut ausgespielt werden.

#### 2. **Leistung des kombinierten Modells:**
- **498 Datenpunkte** wurden von allen Modellen (Text, Audio und kombiniert) korrekt erkannt, was auf eine solide Übereinstimmung zwischen den einzelnen Modalitäten hindeutet.
- **96 Datenpunkte** wurden nur vom kombinierten Modell korrekt erkannt, was darauf hindeutet, dass die Kombination der Modalitäten in bestimmten Fällen zu besseren Ergebnissen führt, wenn beide Einzelmodalitäten nicht ausreichen.
- Es gibt **226 Datenpunkte**, bei denen keines der Modelle korrekt war, was auf die Schwierigkeit dieser Fälle hindeutet, unabhängig von der verwendeten Modalität.

  **Interpretation:** Das kombinierte Modell kann die Stärken der beiden Modalitäten ausnutzen, insbesondere in den Fällen, in denen eine Modalität allein nicht ausreicht. Es gibt jedoch auch Fälle, in denen das kombinierte Modell schlechter abschneidet, zum Beispiel bei den **64 Datenpunkten**, die durch text-only und audio-only korrekt waren, jedoch nicht durch den kombinierten Input. Dies deutet darauf hin, dass die Fusion der Modalitäten in bestimmten Situationen nicht immer vorteilhaft ist.

#### 3. **Vergleich der Modalitäten:**
- **Text vs. Audio:**
  - Das text-only Modell zeigt insgesamt eine deutlich höhere Erkennungsrate als das audio-only Modell. Insbesondere bei den **566 Datenpunkten**, die nur durch Text erkannt wurden, wird der große Einfluss von textbasierten Informationen auf das Ergebnis klar.
  - Das audio-only Modell ist jedoch in der Lage, **172 Datenpunkte** zu erkennen, die das text-only nicht erkennt und **96 Datenpunkte**, die Text-only und das kombinierte Modell nicht korrekt vorhersagen konnten. Dies verdeutlicht, dass es Fälle gibt, in denen akustische Informationen entscheidend sind, das Modell aber nicht in der Lage ist diese Informationen auszunutzen.

- **Kombination der Modalitäten:**
  - Die Kombination von Text und Audio führt zu einer verbesserten Leistung in **96 Fällen**, in denen weder das text-only noch das audio-only Modell alleine erfolgreich war. Dies zeigt den Vorteil eines multimodalen Ansatzes.
  - Allerdings gibt es auch **64 Fälle**, in denen das kombinierte Modell scheitert, obwohl sowohl das Text- als auch das Audio-Modell korrekt waren. Dies könnte darauf hindeuten, dass die Fusion der beiden Modalitäten in diesen Fällen zu einer Informationsüberladung oder zu Konflikten führt.

#### 4. **Stärken und Schwächen der Teilmodelle:**
- Das **text-only Modell** zeigt insgesamt eine hohe Robustheit und liefert die besten Ergebnisse in den meisten Fällen. Es hat den Vorteil Semantik und den vorangegangenen Dialog mit in die Entscheidung einzubeziehen. 
- Das **audio-only Modell** zeigt seine Stärke in speziellen Fällen, in denen akustische Merkmale entscheidend sind. In diesen Fällen kann es Datenpunkte erkennen, die dem text-only Modell verborgen bleiben.
- Das **kombinierte Modell** zeigt seine Stärke besonders in Fällen, in denen die Modalitäten sich gegenseitig ergänzen. In den **96 Fällen**, in denen das kombinierte Modell allein richtig liegt, wird deutlich, dass die Fusion der Modalitäten in komplexeren Situationen von Vorteil ist. Allerdings gibt es auch Fälle, in denen die Kombination zu schlechteren Ergebnissen führt, was auf mögliche Interferenzen oder falsche Gewichtungen zwischen den Modalitäten hinweisen könnte.

```{python}
# 1. Combined Model (C)
df_C = df[combined_model]
print(f"In total, {len(df_C)} samples were recognized by the combined model.")

# 2. Text-Only Model (T)
df_T = df[text_only_model]
print(f"In total, {len(df_T)} samples were recognized by the text-only model.")

# 3. Audio-Only Model (A)
df_A = df[audio_only_model]
print(f"In total, {len(df_A)} samples were recognized by the audio-only model.")

# 4. NOT Combined Model (¬C)
df_not_C = df[not_combined_model]
print(f"In total, {len(df_not_C)} samples were NOT recognized by the combined model.")

# 5. NOT Text-Only Model (¬T)
df_not_T = df[not_text_only_model]
print(f"In total, {len(df_not_T)} samples were NOT recognized by the text-only model.")

# 6. NOT Audio-Only Model (¬A)
df_not_A = df[not_audio_only_model]
print(f"In total, {len(df_not_A)} samples were NOT recognized by the audio-only model.")

# 7. Combined AND Text-Only Models (C ∧ T)
df_C_and_T = df[combined_model & text_only_model]
print(f"In total, {len(df_C_and_T)} samples were recognized by both the combined and text-only models.")

# 8. Combined AND Audio-Only Models (C ∧ A)
df_C_and_A = df[combined_model & audio_only_model]
print(f"In total, {len(df_C_and_A)} samples were recognized by both the combined and audio-only models.")

# 9. Text-Only AND Audio-Only Models (T ∧ A)
df_T_and_A = df[text_only_model & audio_only_model]
print(f"In total, {len(df_T_and_A)} samples were recognized by both the text-only and audio-only models.")

# 10. Combined AND NOT Text-Only Models (C ∧ ¬T)
df_C_and_not_T = df[combined_model & not_text_only_model]
print(f"In total, {len(df_C_and_not_T)} samples were recognized by the combined model but NOT by the text-only model.")

# 11. Combined AND NOT Audio-Only Models (C ∧ ¬A)
df_C_and_not_A = df[combined_model & not_audio_only_model]
print(f"In total, {len(df_C_and_not_A)} samples were recognized by the combined model but NOT by the audio-only model.")

# 12. Text-Only AND NOT Combined Models (T ∧ ¬C)
df_T_and_not_C = df[text_only_model & not_combined_model]
print(f"In total, {len(df_T_and_not_C)} samples were recognized by the text-only model but NOT by the combined model.")

# 13. Text-Only AND NOT Audio-Only Models (T ∧ ¬A)
df_T_and_not_A = df[text_only_model & not_audio_only_model]
print(f"In total, {len(df_T_and_not_A)} samples were recognized by the text-only model but NOT by the audio-only model.")

# 14. Audio-Only AND NOT Combined Models (A ∧ ¬C)
df_A_and_not_C = df[audio_only_model & not_combined_model]
print(f"In total, {len(df_A_and_not_C)} samples were recognized by the audio-only model but NOT by the combined model.")

# 15. Audio-Only AND NOT Text-Only Models (A ∧ ¬T)
df_A_and_not_T = df[audio_only_model & not_text_only_model]
print(f"In total, {len(df_A_and_not_T)} samples were recognized by the audio-only model but NOT by the text-only model.")

# 16. NOT Combined AND NOT Text-Only Models (¬C ∧ ¬T)
df_not_C_and_not_T = df[not_combined_model & not_text_only_model]
print(f"In total, {len(df_not_C_and_not_T)} samples were NOT recognized by both the combined and text-only models.")

# 17. NOT Combined AND NOT Audio-Only Models (¬C ∧ ¬A)
df_not_C_and_not_A = df[not_combined_model & not_audio_only_model]
print(f"In total, {len(df_not_C_and_not_A)} samples were NOT recognized by both the combined and audio-only models.")

# 18. NOT Text-Only AND NOT Audio-Only Models (¬T ∧ ¬A)
df_not_T_and_not_A = df[not_text_only_model & not_audio_only_model]
print(f"In total, {len(df_not_T_and_not_A)} samples were NOT recognized by both the text-only and audio-only models.")

# 19. Combined AND Text-Only AND Audio-Only Models (C ∧ T ∧ A)
df_C_and_T_and_A = df[combined_model & text_only_model & audio_only_model]
print(f"In total, {len(df_C_and_T_and_A)} samples were recognized by the combined, text-only, and audio-only models.")

# 20. Combined AND Text-Only AND NOT Audio-Only Models (C ∧ T ∧ ¬A)
df_C_and_T_and_not_A = df[combined_model & text_only_model & not_audio_only_model]
print(f"In total, {len(df_C_and_T_and_not_A)} samples were recognized by the combined and text-only models but NOT by the audio-only model.")

# 21. Combined AND NOT Text-Only AND Audio-Only Models (C ∧ ¬T ∧ A)
df_C_and_not_T_and_A = df[combined_model & not_text_only_model & audio_only_model]
print(f"In total, {len(df_C_and_not_T_and_A)} samples were recognized by the combined and audio-only models but NOT by the text-only model.")

# 22. Combined AND NOT Text-Only AND NOT Audio-Only Models (C ∧ ¬T ∧ ¬A)
df_C_and_not_T_and_not_A = df[combined_model & not_text_only_model & not_audio_only_model]
print(f"In total, {len(df_C_and_not_T_and_not_A)} samples were recognized by the combined model but NOT by the text-only and audio-only models.")

# 23. NOT Combined AND Text-Only AND Audio-Only Models (¬C ∧ T ∧ A)
df_not_C_and_T_and_A = df[not_combined_model & text_only_model & audio_only_model]
print(f"In total, {len(df_not_C_and_T_and_A)} samples were NOT recognized by the combined model but recognized by both the text-only and audio-only models.")

# 24. NOT Combined AND Text-Only AND NOT Audio-Only Models (¬C ∧ T ∧ ¬A)
df_not_C_and_T_and_not_A = df[not_combined_model & text_only_model & not_audio_only_model]
print(f"In total, {len(df_not_C_and_T_and_not_A)} samples were NOT recognized by the combined and audio-only models but recognized by the text-only model.")

# 25. NOT Combined AND NOT Text-Only AND Audio-Only Models (¬C ∧ ¬T ∧ A)
df_not_C_and_not_T_and_A = df[not_combined_model & not_text_only_model & audio_only_model]
print(f"In total, {len(df_not_C_and_not_T_and_A)} samples were NOT recognized by the combined and text-only models but recognized by the audio-only model.")

# 26. NOT Combined AND NOT Text-Only AND NOT Audio-Only Models (¬C ∧ ¬T ∧ ¬A)
df_not_C_and_not_T_and_not_A = df[not_combined_model & not_text_only_model & not_audio_only_model]
print(f"In total, {len(df_not_C_and_not_T_and_not_A)} samples were NOT recognized by the combined, text-only, and audio-only models.")
```

## Gate analysis

1. wie ist die Verteilung der Gate-Werte?
   - wie ist sie gesamt?
   - wie ist sie für Samples, die nur die Akustik erkannt hat?
2. Robustheitstest
   - Verhalten bei verrauschter Akustik
   - Verhalten bei verrauschtem Text

```{python}
def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

df["softmax_gate"] = df["gate"].apply(lambda x: softmax(x))
df["text_gate_softmax"] = df["softmax_gate"].apply(lambda x: x[0])
df["audio_gate_softmax"] = df["softmax_gate"].apply(lambda x: x[1])
df["text_gate_pred"] = df["gate"].apply(lambda x: sigmoid(x[0]))
df["audio_gate_pred"] = df["gate"].apply(lambda x: sigmoid(x[1]))
df.head(1)
```


Genauigkeit des Gates: 

```{python}
df["gate_audio_correct"] = (
    (df["audio_gate_pred"] > 0.5) & (df["output_audio"] == df["target"])
) | ((df["audio_gate_pred"] < 0.5) & (df["output_audio"] != df["target"]))

df["gate_text_correct"] = (df["text_gate_pred"] > 0.5) & (
    df["output_text"] == df["target"]
) | ((df["text_gate_pred"] < 0.5) & (df["output_text"] != df["target"]))

print("Accuracy on audio-only model:", df["gate_audio_correct"].mean())
print("Accuracy on text-only model:", df["gate_text_correct"].mean())
```

#### Gate robustness

```{python}
import os

import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import AutoConfig, AutoProcessor, AutoTokenizer

from utils.collator import SequenceClassificationCollator
from utils.dataset import ERCDataset, IemocapDataset
from utils.model import MmLlamaConfig, MmLlamaForSequenceClassification
from utils.processor import MmLlamaProcessor

LLM_ID = os.path.abspath("../../models/language/LLaMA2-base")
LLM_ADAPTER = os.path.abspath("../../models/language/adapter/iemocap/LLaMA2-base")
MODEL_PATH = os.path.abspath("../../experiments/multimodal/late_fusion/final/iemocap/LLaMA2-base/final_version/stage_3")
ACOUSTIC_MODEL = os.path.abspath("../../models/acoustic/wav2vec2/wav2vec2-large-robust-12-ft-emotion-msp-dim")
DATASET_PATH = os.path.abspath("../../datasets/iemocap/iemocap.csv")
```

```{python}



def load_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained(LLM_ID)
    tokenizer.pad_token_id = tokenizer.unk_token_id
    tokenizer.padding_side = "left"
    return tokenizer

def load_model_for_test(model: MmLlamaForSequenceClassification):
    model.load_state_dict(
        torch.load(os.path.join(MODEL_PATH, "best_model.pth")), strict=False
    )

    return model.cuda(0)

def setup_config_and_processor(dataset: ERCDataset):
    # Load configurations
    llm_config = AutoConfig.from_pretrained(LLM_ID)
    ac_config = AutoConfig.from_pretrained(ACOUSTIC_MODEL)
    ac_processor = AutoProcessor.from_pretrained(ACOUSTIC_MODEL)

    # setup of tokenizer
    tokenizer = load_tokenizer()

    # setup of processor
    processor = MmLlamaProcessor(ac_processor, tokenizer)

    # setup of config
    config = MmLlamaConfig(
        llm_config=llm_config,
        audio_config=ac_config,
        pad_token_id=tokenizer.pad_token_id,
        llm_pretrained_adapter=LLM_ADAPTER,
        num_labels=len(dataset.emotions),
    )

    return tokenizer, processor, config

def prepare_batch(batch: dict[str, dict[str, torch.Tensor]]):
    for k in batch:
        if type(batch[k]) is torch.Tensor:
            batch[k] = batch[k].cuda(0)
            continue
        for kk in batch[k]:
            batch[k][kk] = batch[k][kk].cuda(0)

    return batch

def run_model(
    model: MmLlamaForSequenceClassification,
    dataloader: DataLoader,
):
    eval_batch_iterator = tqdm(dataloader, total=len(dataloader), desc="Evaluating")
    model.eval()
    for inputs in eval_batch_iterator:
        with torch.no_grad():
            inputs = prepare_batch(inputs)
            model(**inputs)
```

```{python}

# test_dataset = IemocapDataset(
#     DATASET_PATH,
#     mode="test",
#     audio_placement="none",
#     task="normal",
#     window=12,
# )

# tokenizer, processor, config = setup_config_and_processor(test_dataset)

# model = MmLlamaForSequenceClassification(config)
# model = load_model_for_test(model)
```

Applying noise to audio:

```{python}
# all_gates = []


# def gate_hook(module, inputs, outputs: torch.Tensor):
#     global all_gates
#     all_gates.extend(outputs.cpu().tolist())


# def apply_noise(module, args):
#     data, labels = args
#     data = torch.randn_like(data) + torch.mean(data, dim=1, keepdim=True) * torch.std(
#         data, dim=1, keepdim=True
#     )
#     return (data, labels)


# # h1 = model.gate.register_forward_hook(gate_hook)
# # h2 = model.audio_projector.register_forward_pre_hook(apply_noise)

# audio_noise_dl = DataLoader(
#     test_dataset,
#     batch_size=8,
#     shuffle=False,
#     num_workers=8,
#     collate_fn=SequenceClassificationCollator(processor, test_dataset),
# )

# # run_model(model, audio_noise_dl)

# h1.remove()
# h2.remove()

# # df["gate_audio_noise"] = all_gates
```

Applying noise to text:

```{python}
# all_gates = []

# # h1 = model.gate.register_forward_hook(gate_hook)
# # h3 = model.text_projector.register_forward_pre_hook(apply_noise)

# text_noise_dl = DataLoader(
#     test_dataset,
#     batch_size=8,
#     shuffle=False,
#     num_workers=8,
#     collate_fn=SequenceClassificationCollator(processor, test_dataset),
# )

# # run_model(model, text_noise_dl)

# h1.remove()
# h3.remove()

# # df["gate_text_noise"] = all_gates
```

```{python}
# df.to_json(os.path.join(BASE_PATH, DATASET, "iemocap_gates.json"))
df = pd.read_json(os.path.join(BASE_PATH, DATASET, "iemocap_gates.json"))
df.head(1)
```

```{python}
df["text_text_gate_noise_pred"] = df["gate_text_noise"].apply(lambda x: sigmoid(x[0]))
df["audio_text_gate_noise_pred"] = df["gate_text_noise"].apply(lambda x: sigmoid(x[1]))
df["text_audio_gate_noise_pred"] = df["gate_audio_noise"].apply(lambda x: sigmoid(x[0]))
df["audio_audio_gate_noise_pred"] = df["gate_audio_noise"].apply(lambda x: sigmoid(x[1]))
```


```{r}
library(reticulate)
library(ggplot2)
library(ggridges)
library(tidyverse)

s <- seq(0,1,by=0.1)

py$df |> 
  select(contains("gate") & contains("pred")) |> 
  pivot_longer(everything(), names_to = "gate_type", values_to = "prediction") |> 
  group_by(gate_type) |> 
  nest() |> 
  ungroup() |> 
  mutate(
    experiment = rep(c("normal", "text_noise", "audio_noise"), each=2),
    target = rep(c("Text", "Audio"), 3)
  ) |> 
  unnest(data) |> 
  
  ggplot(aes(x = prediction, y = experiment, color = target)) +
    geom_vline(xintercept = 0.5, color = "gray80", linewidth = 1) +
   geom_density_ridges(alpha=0, quantile_lines = F, scale=1, fill="white", linewidth=2) +
    scale_y_discrete(
      limits = rev(c(
        "normal",
        "text_noise",
        "audio_noise"
      )),
      labels=rev(c(
        "Normal",
        "Noise as Text",
        "Noise as Audio"
      ))
    ) +
  scale_color_manual(values = c("#3299FF", "#FF9932"), labels=c("Audio", "Text")) +
  xlab("Prediction") +
  theme_bw() +
  # scale_x_continuous(breaks = s) +
  guides(fill = guide_legend(title = "Target")) +
  theme(axis.title.y = element_blank(),
        axis.title = element_text(size = 26),
        axis.text = element_text(size=24),
        legend.text = element_text(size=24),
        legend.title = element_text(size=26),
        legend.key.size = unit(1, "cm"),
        legend.key.spacing.y = unit(1, "mm"))
  
ggsave("images/gate_noise_iemocap.png", width = 10, height = 10)
  
```
```{r}
gate_df <- bind_rows(
  mutate(py$df[py$audio_only_model & py$combined_model & py$not_text_only_model,], origin="Audio"),
  mutate(py$df[py$not_audio_only_model & py$combined_model & py$text_only_model,], origin="Text"),
  mutate(py$df, origin="All"),
)
gate_df |> 
  select("text_gate_pred", "audio_gate_pred", "origin") |> 
  pivot_longer(!origin, names_to = "Target", values_to = "Prediction") |> 
  ggplot(aes(y = origin, x = Prediction, color = Target)) +
    geom_vline(xintercept = 0.5, color = "gray80", linewidth = 1) +
    geom_density_ridges(alpha=0, quantile_lines = F, scale=1, fill="white", linewidth=2) +
    scale_y_discrete(
      limits = rev(c(
        "All",
        "Text",
        "Audio"
      )),
      labels=rev(c(
        "Full Dataset",
        "Text-Only",
        "Audio-Only"
      ))
    ) +
    scale_color_manual(values = c("#3299FF", "#FF9932"), labels=c("Audio", "Text")) +
    xlab("Prediction") +
    theme_bw() +
    # scale_x_continuous(breaks = s) +
    guides(fill = guide_legend(title = "Target")) +
    theme(axis.title.y = element_blank(),
          axis.title = element_text(size = 26),
          axis.text = element_text(size=24),
          legend.text = element_text(size=24),
          legend.title = element_text(size=26),
          legend.key.size = unit(1, "cm"),
          legend.key.spacing.y = unit(1, "mm"))

ggsave("images/gate_dist_iemocap.png", width = 10, height = 10)

```



Gate auf 0.5 für beide

```{python}
from transformers import LlamaTokenizerFast
import json

def evaluate_fixed_gate(
    tokenizer: LlamaTokenizerFast,
    model: MmLlamaForSequenceClassification,
    dataloader: DataLoader,
):
    eval_batch_iterator = tqdm(dataloader, total=len(dataloader), desc="Evaluating")
    all_targets = []
    all_preds = []
    all_inputs = []

    def gate_hook(module, inputs, outputs:torch.Tensor):
        return torch.empty_like(outputs).fill_(0.5)

    hook = model.gate.register_forward_hook(gate_hook)

    running_loss = 0.0
    model.eval()
    for inputs in eval_batch_iterator:
        labels = inputs["labels"]
        with torch.no_grad():
            try:
                inputs = prepare_batch(inputs)
                preds = model(**inputs)
            except TimeoutError:
                print("TimeoutError on input", inputs)
                preds = torch.zeros((inputs["text"]["input_ids"].size(0), 1))

        running_loss += preds.main_loss
        all_preds.extend(preds.logits.cpu())
        all_targets.extend(labels.cpu())
        all_inputs.extend(inputs["text"]["input_ids"].cpu())

    hook.remove()
    running_loss /= len(dataloader)
    all_preds = torch.stack(all_preds[: len(dataloader.dataset)])
    all_targets = all_targets[: len(dataloader.dataset)]
    all_inputs = all_inputs[: len(dataloader.dataset)]

    all_preds_cert = torch.softmax(all_preds, dim=-1).max(dim=-1).values.tolist()
    all_preds = all_preds.argmax(dim=-1).tolist()
    all_inputs = tokenizer.batch_decode(all_inputs, skip_special_tokens=True)

    dataset = dataloader.dataset
    all_preds = list(map(dataset.id2label, all_preds))
    all_targets = list(map(dataset.id2label, all_targets))

    f1 = f1_score(all_targets, all_preds, average="weighted")
    preds_for_eval = []
    for i, (inp, pred, target, cert) in enumerate(
        zip(all_inputs, all_preds, all_targets, all_preds_cert)
    ):
        preds_for_eval.append(
            {
                "index": i,
                "input": inp,
                "output": pred,
                "target": target,
                "certainty": cert,
            }
        )
   
    with open(os.path.join(BASE_PATH, DATASET, "preds_test_fixed_gate.json"), "wt") as f:
        json.dump(preds_for_eval, f)

    return f1, running_loss
```

```{python}
# fixed_gate_dl = DataLoader(
#     test_dataset,
#     batch_size=8,
#     shuffle=False,
#     num_workers=8,
#     collate_fn=SequenceClassificationCollator(processor, test_dataset),
# )

# f1, loss = evaluate_fixed_gate(tokenizer, model, fixed_gate_dl)
# print(f"F1 in Test: {f1}")
# print(f"Loss in Test: {loss}")
```

```{python}
df_fixed_gate = pd.read_json(os.path.join(BASE_PATH, DATASET, "preds_test_fixed_gate.json")).set_index("index")
df_fixed_gate.head(1)
```

```{python}
f1_score(df_fixed_gate["target"], df_fixed_gate["output"], average="weighted")
```

